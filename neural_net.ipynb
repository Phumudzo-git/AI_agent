{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import pickle as pickle\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigma activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgm(x):\n",
    "    y = 1.0/(1.0+np.exp(-x))\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigma activation function derivative\n",
    "\\begin{align}\n",
    "\\frac{d}{dx}\\sigma(x)=&\\frac{d}{dx}\\bigg(\\frac{1}{1+e^{-x}}\\bigg)\\\\\n",
    "=&\\sigma(x)(1-\\sigma(x))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsgm(x):\n",
    "    return(x*(1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations for the first layer (hidden layer) of neural networks,\n",
    "\\begin{align}\n",
    "z_{1}=& w_1\\cdot x+b_1\\\\\n",
    "\\sigma_{1}(z_1)=&\\sigma(w_1\\cdot x+b_1)\\\\\n",
    "=& \\frac{1}{1+e^{-z_1}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations for the second layer (output layer) of neural networks,\n",
    "\\begin{align}\n",
    "z_{2}=& w_2\\cdot \\sigma_1(z_1)+b_2\\\\\n",
    "\\sigma_{2}(z_2)=&\\sigma(w_2\\cdot \\sigma_1(z_1)+b_2)\\\\\n",
    "=& \\frac{1}{1+e^{-z_2}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_nets(model, image_matrix):\n",
    "    \"\"\" Computing the hidden layer values and the output layer values\"\"\"\n",
    "    hidden_layer = np.dot(model['layer_1']['weight'], image_matrix)+model['layer_1']['bias']\n",
    "    hidden_layer = sgm(hidden_layer)\n",
    "    output_layer = np.dot(hidden_layer, model['layer_2']['weight'])+model['layer_2']['bias']\n",
    "    output_layer = sgm(output_layer)\n",
    "    return(hidden_layer, output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discount function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "R_t=\\sum_{n=0}^H\\gamma^nr_{t+n}\n",
    "\\end{align}\n",
    "\n",
    "where $\\gamma$ is the discounting facter (usually less than 1),  $n$ is the number of timesteps, and $r$ is the magnitude of the reward for a given timestamp $t+n$ where $t$ is the episode time step, and $H$ is the final $n^{th}$ step before the reward is awarded (H is not fixed). The discounts function helps by weighing more rewards actions taken towards the end of an episode and weighing less rewards action taken at the beginning of an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_function(gradient_log_p, ep_rewards, gamma):\n",
    "    \"\"\" Steps taken earlier before the end result are less important to the overall result than the steps taken\n",
    "    recently. This implements that logic by discounting the reward on previous actions based on how long ago they \n",
    "    were taken\"\"\"\n",
    "    discounted_rewards = np.zeros_like(ep_rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, ep_rewards.size)):\n",
    "        if ep_rewards[t] != 0:\n",
    "            running_add = 0 # reset the sum, since this was a game boundary \n",
    "        running_add = running_add * gamma + ep_rewards[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "        \n",
    "    \"\"\" Discount the gradient with the normalized rewards \"\"\"\n",
    "    discounted_episode_rewards = discounted_rewards\n",
    "    # Standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_episode_rewards -= np.mean(discounted_episode_rewards)\n",
    "    discounted_episode_rewards /= np.std(discounted_episode_rewards)\n",
    "    return(gradient_log_p * discounted_episode_rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computation of back propagation we will use the loss function (cost function)\n",
    "\n",
    "\\begin{align}\n",
    "L(y,\\hat{y}) =-\\frac{1}{N}\\sum_{n\\in N}y_n\\log\\hat{y}\n",
    "\\end{align}\n",
    "where $y=\\sigma(z_2)$ is the ground truth and $\\hat{y}$ is the estimated outcome. The deriavative of the loss function with respect the second layer (output layer) weight $w_2$ yields:\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(y,\\hat{y})}{\\partial w_2}=&\\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial\\hat{y}}{\\partial z_2}\\frac{\\partial z_2}{w_2}\\\\\n",
    "=&(y-\\hat{y})*\\frac{\\partial\\hat{y}}{\\partial w_2}*\\sigma(z_1)\\\\\n",
    "=&(y-\\sigma(z_2))*\\sigma^\\prime(z_2)*\\sigma(z_1).\n",
    "\\end{align}\n",
    "\n",
    "The derivative of the loss function with respect the first layer $w_1$ gives,\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(y,\\hat{y})}{\\partial w_1}=&\\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial\\hat{y}}{\\partial z_1}\\frac{\\partial z_1}{w_1}\\\\\n",
    "=&(y-\\hat{y})\\frac{\\partial\\hat{y}}{\\partial z_1}*x\\\\\n",
    "=&(y-\\hat{y})*\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial z_2}{\\partial\\sigma(z_1)}\\frac{\\partial\\sigma(z_1)}{\\partial z_1}*x\\\\\n",
    "=&(y-\\hat{y})*\\sigma^{\\prime}(z_2)*w_2*\\sigma^{\\prime}(z_1)*x\n",
    "\\end{align}\n",
    "where $x$ will be the image matrix (array).\n",
    "\n",
    "The derivatives for the cost function with respect the bias terms from the first and second layers are respectively:\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(y,\\hat{y})}{\\partial b_2}=&\\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial\\hat{y}}{\\partial z_2}\\frac{\\partial z_2}{b_2}\\\\\n",
    "=&(y-\\hat{y})*\\frac{\\partial\\hat{y}}{\\partial b_2}\\\\\n",
    "=&(y-\\sigma(z_2))*\\sigma^\\prime(z_2).\\\\\n",
    "\\frac{\\partial L(y,\\hat{y})}{\\partial b_1}=&\\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial\\hat{y}}{\\partial z_1}\\frac{\\partial z_1}{w_1}\\\\\n",
    "=&(y-\\hat{y})*\\frac{\\partial \\hat{y}}{\\partial z_2}\\frac{\\partial z_2}{\\partial\\sigma(z_1)}\\frac{\\partial\\sigma(z_1)}{\\partial z_1}*x\\\\\n",
    "=&(y-\\hat{y})*\\sigma^{\\prime}(z_2)*w_2*\\sigma^{\\prime}(z_1)\n",
    "\\end{align}\n",
    "\n",
    "The weights and bias parameters will be updated using RMSProp. The update RMSProp for the weights parameters will be,\n",
    "\\begin{align}\n",
    "E^w[g^2]_t=&\\beta *E^w[g^2]_{t-1}+(1-\\beta)(g_t)^2,\\quad\\quad\\beta =0.9\\\\\n",
    "=&0.9*E^w[g^2]_{t-1}+0.1g_t^2\\\\\n",
    "w_{t+1}=&w_t-\\frac{\\eta}{E^w[g^2]_t+\\epsilon}g_t\n",
    "\\end{align}\n",
    "And the update RMSProp for the bias parameters will be,\n",
    "\\begin{align}\n",
    "E^b[g^2]_t=& \\beta *E^b[g^2]_{t-1}+(1-\\beta)g_t^2,\\quad\\quad\\beta =0.9\\\\\n",
    "=& 0.9 *E^b[g^2]_{t-1}+0.1*g_t^2\\\\\n",
    "b_{t+1}=&b_t-\\frac{\\eta}{E^b[g^2]_t+\\epsilon}g_t\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(weight_gradients, hidden_layers,output, image_matrices, decay_rate, learning_rate, model,g_dict,e_dict):\n",
    "    ep_number=0   \n",
    "    batch_size = 10\n",
    "    epsilon = 1e-5\n",
    "    delta_L = weight_gradients\n",
    "    dW_2 = np.dot(hidden_layers.T, (delta_L* dsgm(output))).ravel()\n",
    "    delta_l2 = np.outer(delta_L,model['layer_2']['weight'])\n",
    "    delta_l2 = delta_l2*dsgm(hidden_layers)\n",
    "    dW_1 =  np.dot((np.outer(delta_L * dsgm(output),model['layer_2']['weight'].T)* dsgm(hidden_layers)).T, image_matrices)\n",
    "    db_1 = np.sum(np.outer(delta_L * dsgm(output),model['layer_2']['weight'].T)* dsgm(hidden_layers),axis=0)\n",
    "    db_2 = np.sum(delta_L* dsgm(output),axis=0)\n",
    "    \n",
    "    \"\"\"Updating the weights\"\"\"\n",
    "    g_dict['layer_1']['weight']+= dW_1\n",
    "    g_dict['layer_2']['weight']+= dW_2\n",
    "    g_dict['layer_1']['bias']+= db_1\n",
    "    g_dict['layer_2']['bias']+= db_2\n",
    "    \n",
    "    \"\"\"Applying RMSprop\"\"\"\n",
    "    if ep_number % batch_size==0:\n",
    "        for weight in model:\n",
    "            g_t = g_dict[weight]['weight']\n",
    "            g_b = g_dict[weight]['bias']\n",
    "            e_dict[weight]['weight'] = decay_rate*e_dict[weight]['weight']+(1-decay_rate)*g_t**2\n",
    "            e_dict[weight]['bias'] = decay_rate*e_dict[weight]['bias']+(1-decay_rate)*g_b**2\n",
    "            model[weight]['weight']+=(learning_rate*g_t)/(np.sqrt(e_dict[weight]['weight'] + epsilon))\n",
    "            model[weight]['bias']+=(learning_rate*g_b)/(np.sqrt(e_dict[weight]['bias'] + epsilon))\n",
    "            g_dict[weight]['weight']=np.zeros_like(model[weight]['weight'])\n",
    "            g_dict[weight]['bias']=np.zeros_like(model[weight]['bias'])\n",
    "            \n",
    "    return(model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing function:\n",
    "The image pre-processing function helps with croping the image and leaving only the components that are important for learning the game. After that the image will be down sampled, then the color will be reduced to only black and white and the background will be removed. The image will then be flattened, by converting it from an 80X80 array (matrix) into a 6400X1 vector, this will make feed forward pass of the neural network convenient. The function will then store the difference between succesive image frames, since the difference between images will capture the motion and direction of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_images(input_observation, prev_processed_observation, input_dimensions):\n",
    "    \"\"\" Here we want to convert the image of 210x160x3 array into a 6400 float vector \"\"\"\n",
    "    processed_observation = input_observation[35:195] # crop\n",
    "    processed_observation = down_sample(processed_observation) # Downsample function below below\n",
    "    processed_observation = remove_color(processed_observation) # Remove color function implemented below\n",
    "    processed_observation = remove_background(processed_observation) # Remove background function implemented below\n",
    "    processed_observation[processed_observation != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    \"\"\"Now we convert the 80X80 array (matrix) image into 1600X1 matrix\"\"\"\n",
    "    processed_observation = processed_observation.astype(np.float).ravel()\n",
    "\n",
    "    \"\"\"Here we process the difference in succesive frames. Subtract the previous image frame from the \n",
    "    current frame \"\"\"\n",
    "    if prev_processed_observation is not None:\n",
    "        input_observation = processed_observation - prev_processed_observation\n",
    "    else:\n",
    "        input_observation = np.zeros(input_dimensions)\n",
    "    \"\"\" Save the previous image frame \"\"\"\n",
    "    prev_processed_observations = processed_observation\n",
    "    return(input_observation, prev_processed_observation)\n",
    "\n",
    "def down_sample(image):\n",
    "    return image[::2, ::2, :]\n",
    "\n",
    "def remove_color(image):\n",
    "    return image[:, :, 0]\n",
    "\n",
    "def remove_background(image):\n",
    "    image[image == 144] = 0\n",
    "    image[image == 109] = 0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability function\n",
    "\n",
    "The feed forward output layer (second layer) will output a probability number from the sigmoid activation function, the number will be between 0 and 1, this probability function will be input into a choose_action, the choose_action function will will either return a value 2 or 3, depending on the probability value. The reuturned value from the choose action will be used to move the AI agent up either up or down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(probability):\n",
    "    random_value = np.random.uniform()\n",
    "    if random_value < probability:\n",
    "        return 2      # the AI agent will move up in the openai gym\n",
    "    else:\n",
    "        return 3      # The AI agent will move down in the openai gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume = False # resume from previous checkpoint?\n",
    "D = 80 * 80\n",
    "H = 200 \n",
    "# model= {'layer_1':{},'layer_2':{}}\n",
    "# model['layer_1']['weight']=np.random.randn(H,D) / np.sqrt(D)\n",
    "# model['layer_1']['bias']=np.random.randn(H) / np.sqrt(H)\n",
    "# model['layer_2']['weight']=np.random.randn(H) / np.sqrt(H)\n",
    "# model['layer_2']['bias']=np.random.randn(1) / np.sqrt(H)\n",
    "model = pickle.load(open('model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function\n",
    "First, a collection of parameters will be set before the model (Main function) can be run. These parameters are required for the forward propagation and updating of the weights during backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    image = env.reset() # This gets us the image\n",
    "    epoch=50\n",
    "\n",
    "    \"\"\"Parameters\"\"\"\n",
    "    episode_number = 0\n",
    "    batch_size = 10\n",
    "    gamma = 0.99 # This is the discount factor for rewards\n",
    "    decay_rate = 0.99\n",
    "    hidden_layer_neurons = 200\n",
    "    input_dimensions = 80 * 80\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    ep_number = 0\n",
    "    reward_sum = 0\n",
    "    running_reward = None\n",
    "    prev_process_images = None\n",
    "\n",
    "    \"\"\"Dictionary to store RMSprop weights\"\"\"\n",
    "    e_dict = {'layer_1':{},'layer_2':{}}\n",
    "    g_dict = {'layer_1':{},'layer_2':{}}\n",
    "    for layer_name in model:\n",
    "        e_dict[layer_name]['weight'] = np.zeros_like(model[layer_name]['weight'])\n",
    "        e_dict[layer_name]['bias'] = np.zeros_like(model[layer_name]['bias'])\n",
    "        g_dict[layer_name]['weight'] = np.zeros_like(model[layer_name]['weight'])\n",
    "        g_dict[layer_name]['bias'] = np.zeros_like(model[layer_name]['bias'])\n",
    "        \n",
    "\n",
    "    ep_hidden_layers, ep_images, ep_weight_gradients, ep_rewards = [], [], [], []\n",
    "\n",
    "\n",
    "    while epoch:\n",
    "        env.render()\n",
    "        process_images, prev_process_images = pre_process_images(image, prev_process_images, input_dimensions)\n",
    "        hidden_layers, probability = feed_forward_nets(model,process_images)\n",
    "    \n",
    "        ep_images.append(process_images)\n",
    "        ep_hidden_layers.append(hidden_layers)\n",
    "\n",
    "        action = choose_action(probability)\n",
    "\n",
    "        \"\"\"Action for up or down\"\"\"\n",
    "        image, reward, done, info = env.step(action)\n",
    "\n",
    "        reward_sum += reward\n",
    "        ep_rewards.append(reward)\n",
    "\n",
    "        made_label = 1 if action == 2 else 0\n",
    "        loss_function = made_label - probability\n",
    "        ep_weight_gradients.append(loss_function)\n",
    "\n",
    "\n",
    "        if done: # when an eposode is done\n",
    "            ep_number += 1\n",
    "\n",
    "            \"\"\"Combine the episode images, hidden layers values, weights/gradients and also the rewards for \n",
    "            the actions in each episodes\"\"\"\n",
    "            ep_hidden_layers = np.vstack(ep_hidden_layers)\n",
    "            ep_images = np.vstack(ep_images)\n",
    "            ep_weight_gradients = np.vstack(ep_weight_gradients)\n",
    "            ep_rewards = np.vstack(ep_rewards)\n",
    "\n",
    "            \"\"\"Applying discount function\"\"\"\n",
    "            ep_weight_gradients_discount = discount_function(ep_weight_gradients, ep_rewards, gamma)\n",
    "\n",
    "            gradient = back_propagation(ep_weight_gradients_discount,ep_hidden_layers,probability,ep_images,decay_rate, learning_rate,model\n",
    "                                ,g_dict,e_dict)\n",
    "            \n",
    "            \"\"\"Reset the episode stored information by cleaning the lists above\"\"\"\n",
    "            ep_hidden_layers, ep_images, ep_weight_gradients, ep_rewards = [], [], [], [] \n",
    "            image = env.reset() # Also reset the env gym\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            print('Resetting the env. The episode reward total is {}. The running mean is:{}'.format(reward_sum, running_reward))\n",
    "            reward_sum = 0\n",
    "            prev_process_images = None\n",
    "            print(epoch)\n",
    "            epoch-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main()#200,80*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
